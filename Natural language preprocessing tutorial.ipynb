{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Natural language preprocessing tutorial\n",
    "\n",
    "Sometimes, we will get some data with natural language or just string variables, then we should come up with a solution to solve these data, as most algorithms don't support `string` variables except tree based algorithms like `Decision Tree` and `Random Forest`, but I really recommend to process these string columns to be numerical, then you chould try more algorithms that could fit on these data!\n",
    "\n",
    "This tutorial will focus on the string variable and natural language preprocessing to make the data to be numerical for later algorithms, this will cover bellow algorithms:\n",
    "\n",
    "1. LabelEncoder\n",
    "2. OneHotEncoder\n",
    "3. Bag-of-Words\n",
    "4. N-Grams\n",
    "5. TFIDF\n",
    "\n",
    "I will explain this algorithms one by one also with code.\n",
    "\n",
    "Let's go :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "import warnings\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder, OneHotEncoder\n",
    "\n",
    "style.use('ggplot')\n",
    "\n",
    "# don't want to see the wanrings info\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "      <th>sex</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I like bed</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I like baseball</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>baseball is my life</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>bed is great</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I play baseball</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id             comments     sex label\n",
       "0  1           I like bed  female     0\n",
       "1  2     I like baseball     male     1\n",
       "2  3  baseball is my life    male     0\n",
       "3  4         bed is great  female     1\n",
       "4  5      I play baseball    male     1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here I just make a toy dataset\n",
    "user_id = np.array([1, 2, 3, 4, 5])\n",
    "sex_cols = np.array(['female', 'male', 'male', 'female', 'male'])\n",
    "comments = np.array([\"I like bed\", \"I like baseball \", \"baseball is my life\", \"bed is great\", \"I play baseball\"])\n",
    "labels = np.array([0, 1, 0, 1, 1])\n",
    "\n",
    "data = np.concatenate([user_id[:, np.newaxis], comments[:, np.newaxis], sex_cols[:, np.newaxis], labels[:, np.newaxis]], axis=1)\n",
    "\n",
    "df = pd.DataFrame(data, columns=['id', 'comments', 'sex', 'label'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LabelEncoder\n",
    "\n",
    "Supporse you are given a data set that contains users' info try to predict whether users like to do sports, as you have the goal that try to predict a new comer whether or not like doing sports, then you should first process your data. \n",
    "\n",
    "You find that you have one column called `Sex`, that contains just `male` and `female`, as I have ever recommended that you should convert the string variable to numbers. But how to do make it? The really basic thought is just to make `male` to 0 and make `female` to 1, right? That's **LabelEncoder**! so the logic is really easy, I have read the `sklearn` source code, it just get the unique string, sort it, give the continous integer for each string! Then you could get the string converted result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted label:  [0 1 1 0 1]\n",
      "we could even just with fit_transform, with same result: [0 1 1 0 1]\n"
     ]
    }
   ],
   "source": [
    "# we could make the sex columns with label encoder, to make male to 0 and female to 1\n",
    "le = LabelEncoder()\n",
    "\n",
    "le.fit(df['sex'])\n",
    "\n",
    "print(\"Converted label: \", le.transform(df['sex']))\n",
    "\n",
    "print(\"we could even just with fit_transform, with same result:\", le.fit_transform(df['sex']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result with label binarizer: [[0]\n",
      " [1]\n",
      " [1]\n",
      " [0]\n",
      " [1]]\n",
      "Demension of trnasform result:  (5, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comments</th>\n",
       "      <th>sex</th>\n",
       "      <th>label</th>\n",
       "      <th>sex_lb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>I like bed</td>\n",
       "      <td>female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I like baseball</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>baseball is my life</td>\n",
       "      <td>male</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>bed is great</td>\n",
       "      <td>female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>I play baseball</td>\n",
       "      <td>male</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id             comments     sex label  sex_lb\n",
       "0  1           I like bed  female     0       0\n",
       "1  2     I like baseball     male     1       1\n",
       "2  3  baseball is my life    male     0       1\n",
       "3  4         bed is great  female     1       0\n",
       "4  5      I play baseball    male     1       1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in fact, we could do the same thing with label binarizer with more efficient way\n",
    "# as LabelBinarizer will return with len(sample) * 1\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "lb.fit(df['sex'])\n",
    "\n",
    "print(\"result with label binarizer:\", lb.transform(df['sex']))\n",
    "print(\"Demension of trnasform result: \", lb.transform(df['sex']).shape)\n",
    "\n",
    "# then we could just add the new columns with original dataframe\n",
    "lb_df = pd.DataFrame(lb.transform(df['sex']), columns=['sex_lb'])\n",
    "\n",
    "# combine two data frames\n",
    "df = pd.concat([df, lb_df], axis=1)\n",
    "\n",
    "# show the new dataframe\n",
    "# as you could see that we do make the sex column to be numerical\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHotEncoder\n",
    "\n",
    "In fact, we have introducted **OneHotEncoder** before, please reference to this: [Preprocessing tutorial](https://github.com/lugq1990/machine-learning-beginner-jupyter-series/blob/master/preprocessing%20turorial.ipynb), the reason that we should use **OneHotEncoder** is that if we just convert the sex to be number 0 and 1, we really bring some bias for the model, the reason is that for `sex` column, we should give different importance, as the 1 is larger than 0, if we just fit the data with linear model like Logistic Regression, if the sample label is 1, then for the sample `sex` with 1, then the weights should be 1, right? but for the `sex` with 0, no matter how big the weights is, we don't make it to 1, right? But we don't just fit one feature, but in fact, we do really change the importance for each sample! This isn't right! \n",
    "\n",
    "That's coming with **OneHotEncoder**, onehot could make the continous number to be a vector, suppose you have a data with 0 and 1 as the previous `sex` column, then by using **OneHotEncoder**, we could convert 0 to (0, 1) and 1 to (1, 0), if you have n unique variables, then you will get the n Dimension vector, then will only one position will be 1, others will be 0, the 1 position is means which value means, by using **OneHotEncoder**, we could remove the order that we come with converted with **LabelBinerizer**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converted OneHot: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<5x2 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = OneHotEncoder()\n",
    "\n",
    "# we have to ensure the data with at least 2D, convert the data (n,) to (n, 1)\n",
    "sex_lb = df['sex_lb'].values[:, np.newaxis]\n",
    "onehot.fit(sex_lb)\n",
    "\n",
    "# then we could convert the number to onehot vector\n",
    "print(\"converted OneHot: \", )\n",
    "onehot.transform(sex_lb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [0., 1.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# as you could see that the one-hot converted result is a sparse matrix, \n",
    "# the reason is using the sparse matrix is more efficient for matrix with many 0\n",
    "\n",
    "# we could get the info, then you see that for each row will 2D\n",
    "onehot.transform(sex_lb).todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bag-of-words\n",
    "\n",
    "When there are natural language, then we need to convert the string to numbers to represent the info in the sentence, there are really many use cases for natural language, like sentiment analysis, there are just the comments that users express themself with words, just like me. \n",
    "\n",
    "But how could we represent the words? The really easy parts is just **bag-of-words**, the logic is really easy, supporse you have two sentences: \"I do machine learning\", \"I like learning\", then **bag-of-words** will get the whole unique words are: (\"I\", \"do\", \"machine\", \"learning\", \"like\"), there are just 5 unique words, then you could just construct your data with 2 * 5 D, 2 means 2 sentences, 5 means there are just 5 unique words, then you will just count how many words in each sentence, if exists, add 1, otherwise 0. Sample data will be (1, 1, 1, 1, 0), (1, 0, 0, 1, 1), how about there are same words in each sentence, just cound how many!\n",
    "\n",
    "If we face real world massy data, we will face more complexy sentence, we will have to do other things, I will write other tutorials for NLP, keep tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get sentence: ['I like bed' 'I like baseball ' 'baseball is my life' 'bed is great'\n",
      " 'I play baseball']\n",
      "Get bag of words matrix: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0, 1, 0, 0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 1, 0, 0],\n",
       "        [1, 0, 0, 1, 1, 0, 1, 0],\n",
       "        [0, 1, 1, 1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow = CountVectorizer()\n",
    "\n",
    "# get the comments data \n",
    "comments = df['comments'].values\n",
    "\n",
    "print(\"Get sentence:\", comments)\n",
    "\n",
    "# fit the model\n",
    "bow.fit(comments)\n",
    "\n",
    "bow_matrix = bow.transform(comments)\n",
    "\n",
    "print(\"Get bag of words matrix: \")\n",
    "# result is also a sparse matrix\n",
    "bow_matrix.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baseball', 'bed', 'great', 'is', 'life', 'like', 'my', 'play']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# in fact, we could get the unique words in sentence. \n",
    "# But one thing you should notice that, unique doesn't contain words \"I\", \n",
    "# as the CounterVectorizer will remove the stop words from the sentence\n",
    "bow.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more words for **stop words**: as the sentence may contain many words that don't contain any info like \"I\", \"she\", etc. these words are called **stop words**, we should remove it if we do the data clearning, I will show you in future NLP tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### N-Gram\n",
    "\n",
    "For **N-Gram** is just to add more info for our data, in fact, if we just get words from sentences, we don't take much info, as some phrases are just removed for us, so this comes out with **N-Gram**! N is how big window we should take to take phrases, suppose you have one sentence: \"I love machine learning\", then you let N=2(called Bigram), then the result will be (\"I love\", \"love machine\", \"machine learning\"), so that we do take more info by using **N-Gram**, right? In fact, you could make N to 3 or 4, but one thing to notice that: if you give too big N, then you will have more features! Take care, I just recommend N to be 2 or 3 will be enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram = CountVectorizer(ngram_range=(1, 2))\n",
    "\n",
    "ngram.fit(comments)\n",
    "\n",
    "gram_two = ngram.transform(comments)\n",
    "\n",
    "# the 2 grams is also a sparse matrix, we could convert it to dense matrix to show\n",
    "gram_two.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more words for **N-gram**, as you could notice that I just set `CountVectorizer`'s `ngram_range` (1, 2), the meaning is that we both take 1-gram and 2 gram, so that we could keep more info in the data, both with words and phrases! Real work should consider this!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF\n",
    "\n",
    "When we process data with many words and big datasets, then we want to make different importance for each words in each sentence, the reason is that for each sentence, some words are important called `key words`, so want our algorithm to keep more focus on this `key words`, in fact the `Attension` algorithm is also the same logic!\n",
    "\n",
    "But how do we compute the **TF-IDF**? the solution is really easy, there are two parts: `TF` means term frequency as how many times the same words appear in each sentence, `IDF` means inverse documents frequency means if a word like \"I\" appears in the whole documents, then word \"I\" don't contain any info. \n",
    "\n",
    "So **TF-IDF** is just with (# words / # whole words) * (log((1+ #documents) / (1 + # words apear in documents)))!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get matrix shape: (5, 8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[0.        , 0.70710678, 0.        , 0.        , 0.        ,\n",
       "         0.70710678, 0.        , 0.        ],\n",
       "        [0.63871058, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.76944707, 0.        , 0.        ],\n",
       "        [0.38040565, 0.        , 0.        , 0.45827018, 0.56801408,\n",
       "         0.        , 0.56801408, 0.        ],\n",
       "        [0.        , 0.53177225, 0.659118  , 0.53177225, 0.        ,\n",
       "         0.        , 0.        , 0.        ],\n",
       "        [0.55645052, 0.        , 0.        , 0.        , 0.        ,\n",
       "         0.        , 0.        , 0.83088075]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# just convert the sentence directly\n",
    "tfidf_data = tfidf.fit_transform(comments)\n",
    "\n",
    "print(\"Get matrix shape:\", tfidf_data.shape)\n",
    "# we have to make it to dense\n",
    "tfidf_data.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['baseball', 'bed', 'great', 'is', 'life', 'like', 'my', 'play']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we could get the unique words for TFIDF\n",
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final words\n",
    "\n",
    "This tutorial contains some basic idea to convert the sentence to numerical values, in fact, there are some more efficient way to extract the info by using `Deep Learning`, like `Word2Vec`, `Sentence2Vec`, `Paragraph2Vec`, even `Object2Vec`! I will cover these in future tutorials!\n",
    "\n",
    "Keep tuned! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
